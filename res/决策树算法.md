# 决策树(Decision Tree)

从树根节点到每一个叶节点都是一个决策，叶节点表示决策的结果

从数据集中学习到**最优**的决策树有下面的几种方法

## ID3算法

利用最大信息增益(Information Gain)的方法，每次决定子节点的时候选择能够使信息增益最大的属性(attribute)，也就是能够使熵减最大的属性，该算法属于**贪心**算法，具体步骤如下：

假设当前样本集$D$的第k个样本概率为$p_k$，那么信息熵$Ent(D)$的定义为：

$$Ent(D)=-\sum^{|D|}_{k=1}p_klog_2p_k$$

假设离散属性集$A$，对每个$a\in A$，$a$的可能取值为$\{a_1,a_2,a_3,...,a_V\}$，用$a$对样本$D$进行划分得到$v$个分支节点，把第v个分支节点记作$D_v$，定义该分支节点的权重为$\frac{|D^v|}{|D|}$，那么属性$a$对样本集$D$划分得到的信息增益定义为：

$Gain(D,a)=Ent(D)-\sum^v_{v=1}{\frac{|D^v|}{|D|}Ent(D^v)}$

将信息增益最大的属性当作树的当前节点$n$：

$n=argmax_{arg=a}a(Gain(D,a))$

确定当前节点后对每个分支节点$D_v$进行递归操作，直到该分支的信息熵$Ent(D)$为零。

### 缺点

* 在选择子节点时偏向于取值数目较多的属性，且只能处理离散属性。

### 如何解决过拟合问题

可通过设定新的优化目标：$conplexity(f(x)) + accuracy(f(x))$来解决，其中$complexity(f(x))$表示决策树的复杂程度（一般用Minimum Description Length来描述），$accuracy(f(x))$，表示决策树的准确度，新的优化目标目的是在二者之间找到平衡点（解决过拟合问题的一般性思路）。

具体做法是先将决策树构建出来，然后在新的优化目标下进行剪枝，直到达到最优。

### [Python实现](../src/decision_tree.py)

## C4.5算法

和ID3算法类似，只不过将信息增益改为**信息增益比**，能够解决之前提到的选择子节点时偏向于取值数目较多较多的属性问题。

信息增益比$Gain\_ratio$的定义：

$Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$

其中$IV(a)$用来衡量属性$a$的取值数目，数目越多，$IV(a)$越大，具体表示为：

$IV(a)=-\sum^V_v{\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}}$

其实就是属性的样本集的熵

### 过拟合问题

过拟合问题也可以像ID3算法一样进行剪枝

<!-- ### TODO: Python代码实现 -->

## CART算法

<!-- TODO: CART算法 -->
